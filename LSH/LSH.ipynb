{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T11:26:55.548178Z",
     "start_time": "2019-12-11T11:26:55.542196Z"
    }
   },
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import glob # to get file paths\n",
    "import sympy # to check prime number\n",
    "import random\n",
    "import numpy as np # only to get prime number list\n",
    "import itertools # to generate pairs from list\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T11:26:57.738195Z",
     "start_time": "2019-12-11T11:26:57.714290Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=LSH, master=local) created by __init__ at <ipython-input-6-23aa47e2de05>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-498-23aa47e2de05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LSH\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.default.parallelism\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mA:\\Anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mA:\\Anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    330\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 332\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    333\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=LSH, master=local) created by __init__ at <ipython-input-6-23aa47e2de05>:2 "
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"LSH\").set(\"spark.default.parallelism\", 4)\n",
    "sc = SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:14:39.170793Z",
     "start_time": "2019-12-11T12:14:39.164842Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameter\n",
    "## K-Shingle\n",
    "K = 3\n",
    "## hash functions\n",
    "hash_functions = 100\n",
    "a_max = 10000\n",
    "b_max = 10000\n",
    "SEED = 0\n",
    "## LSH\n",
    "bands = 50\n",
    "rows = 2\n",
    "TopK = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read files into a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 讀取所有50個txt檔加到一個list裡面\n",
    "+ 對於每一個檔案的內容過濾掉`\\` `.` `\\n` `,` `-`等符號，並都轉成小寫\n",
    "+ 然後以list的方式存入DataSet內"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:14:43.002455Z",
     "start_time": "2019-12-11T12:14:40.834098Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./athletics\\001.txt\n",
      "./athletics\\002.txt\n",
      "./athletics\\003.txt\n",
      "./athletics\\004.txt\n",
      "./athletics\\005.txt\n",
      "./athletics\\006.txt\n",
      "./athletics\\007.txt\n",
      "./athletics\\008.txt\n",
      "./athletics\\009.txt\n",
      "./athletics\\010.txt\n",
      "./athletics\\011.txt\n",
      "./athletics\\012.txt\n",
      "./athletics\\013.txt\n",
      "./athletics\\014.txt\n",
      "./athletics\\015.txt\n",
      "./athletics\\016.txt\n",
      "./athletics\\017.txt\n",
      "./athletics\\018.txt\n",
      "./athletics\\019.txt\n",
      "./athletics\\020.txt\n",
      "./athletics\\021.txt\n",
      "./athletics\\022.txt\n",
      "./athletics\\023.txt\n",
      "./athletics\\024.txt\n",
      "./athletics\\025.txt\n",
      "./athletics\\026.txt\n",
      "./athletics\\027.txt\n",
      "./athletics\\028.txt\n",
      "./athletics\\029.txt\n",
      "./athletics\\030.txt\n",
      "./athletics\\031.txt\n",
      "./athletics\\032.txt\n",
      "./athletics\\033.txt\n",
      "./athletics\\034.txt\n",
      "./athletics\\035.txt\n",
      "./athletics\\036.txt\n",
      "./athletics\\037.txt\n",
      "./athletics\\038.txt\n",
      "./athletics\\039.txt\n",
      "./athletics\\040.txt\n",
      "./athletics\\041.txt\n",
      "./athletics\\042.txt\n",
      "./athletics\\043.txt\n",
      "./athletics\\044.txt\n",
      "./athletics\\045.txt\n",
      "./athletics\\046.txt\n",
      "./athletics\\047.txt\n",
      "./athletics\\048.txt\n",
      "./athletics\\049.txt\n",
      "./athletics\\050.txt\n"
     ]
    }
   ],
   "source": [
    "DataSet = []\n",
    "\n",
    "filepath=glob.glob('./athletics/*.txt')\n",
    "for file in filepath:\n",
    "    #開啟檔案\n",
    "    print(file)\n",
    "    with open(file, \"r\") as f:    \n",
    "        #讀取檔案\n",
    "        text = f.read()\n",
    "        # 過濾符號 轉小寫 \n",
    "        text = text.replace(\"\\\"\",\"\").replace(\".\",\"\").replace(\"\\n\",\" \").replace(\",\",\"\").replace(\"-\",\" \").lower()\n",
    "        # 分詞\n",
    "        text = text.split(\" \")\n",
    "        \n",
    "        # 過濾空詞\n",
    "        while \"\" in text:\n",
    "            text.remove(\"\")\n",
    "        \n",
    "        # 以list格式存在list內\n",
    "        DataSet.append(text)\n",
    "#         print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make list into RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 將DataSet轉換成RDD的格式方便Spark處理\n",
    "+ 加上index表示不同的documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:14:44.889463Z",
     "start_time": "2019-12-11T12:14:44.865497Z"
    }
   },
   "outputs": [],
   "source": [
    "DataRDD = sc.parallelize(DataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:14:47.454962Z",
     "start_time": "2019-12-11T12:14:45.119349Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加上index方便處理\n",
    "Data_with_index = DataRDD.zipWithIndex().map(lambda x: (x[1]+1,x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Shingle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 將一個document RDD內的list 以K=3的方式產生Shingle到一個set內(用來去除重複內容)\n",
    "+ 對於set內每個3-Shingle hash 產生一個 `Shingle-ID`(因為我們只在意相似度，不在意原本的內容)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:14:47.515765Z",
     "start_time": "2019-12-11T12:14:47.507787Z"
    }
   },
   "outputs": [],
   "source": [
    "def k_shingle (RDD):\n",
    "    text_list = RDD[1]\n",
    "    \n",
    "    size = len(text_list) - K + 1\n",
    "    shingle_list = []\n",
    "#     for key,value in enumerate(text_list):\n",
    "#         print(text_list[key])\n",
    "    for i in range(size):\n",
    "#         print(text_list[i],text_list[i+1],text_list[i+2])\n",
    "        shingle_list.append([text_list[i],text_list[i+1],text_list[i+2]])\n",
    "    \n",
    "    shingle_set = set(tuple(item) for item in shingle_list)\n",
    "    return(RDD[0],shingle_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:14:47.554660Z",
     "start_time": "2019-12-11T12:14:47.549674Z"
    }
   },
   "outputs": [],
   "source": [
    "def mapper1 (RDD):\n",
    "    maplist = []\n",
    "    for item in RDD[1]:\n",
    "        maplist.append((RDD[0],hash(item)))\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:14:47.607522Z",
     "start_time": "2019-12-11T12:14:47.602533Z"
    }
   },
   "outputs": [],
   "source": [
    "Shingle_Data = Data_with_index.map(k_shingle).flatMap(mapper1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `N`是Shingle的總數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:14:54.832120Z",
     "start_time": "2019-12-11T12:14:50.078179Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12900"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Shingle_count = Shingle_Data.map(lambda x : (x[1],1)).reduceByKey(lambda x,y : x+y)\n",
    "N = Shingle_count.count()\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `P`是大於N的質數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:14:54.896914Z",
     "start_time": "2019-12-11T12:14:54.890931Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prime_number(n):\n",
    "    N = n if n%2 else n+1\n",
    "    N *= 43\n",
    "    while not sympy.isprime(N):\n",
    "        N += 2\n",
    "    return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:14:54.944787Z",
     "start_time": "2019-12-11T12:14:54.936810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554747"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = get_prime_number(N)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `a,b`是隨機產生的整數數列，一共100個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:14:57.507592Z",
     "start_time": "2019-12-11T12:14:57.502607Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\Anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: This function is deprecated. Please call randint(0, 10000 + 1) instead\n",
      "  \n",
      "A:\\Anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: This function is deprecated. Please call randint(0, 10000 + 1) instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "a = np.random.random_integers(0, a_max, hash_functions)\n",
    "b = np.random.random_integers(0, b_max, hash_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Shingle-ID with 100 hash functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H_{a,b}(x) = ((ax+b)mod P)mod N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 100個`hash function`產生出來長度為100的`hash value`數列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:15:00.318531Z",
     "start_time": "2019-12-11T12:15:00.313543Z"
    }
   },
   "outputs": [],
   "source": [
    "Hash_Data = Shingle_Data.map(lambda x : (x[0], (x[1]*a+b)%P%N)) # i,[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Min-Hash values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 用`min_list`reduce來取得100個hash function分別算出來的最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:15:01.102161Z",
     "start_time": "2019-12-11T12:15:01.097179Z"
    }
   },
   "outputs": [],
   "source": [
    "def min_list(a,b):\n",
    "    for i in range(hash_functions):\n",
    "        a[i] = a[i] if a[i]<b[i] else b[i]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:15:01.853324Z",
     "start_time": "2019-12-11T12:15:01.830353Z"
    }
   },
   "outputs": [],
   "source": [
    "Min_Hash_Data = Hash_Data.reduceByKey(lambda x,y : min_list(x,y))# i,[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality-Sensitive Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get candidate pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signature matrix devided by bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 將原本的長度100的`min-hash value` map到 50個不同的`band`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:15:05.582318Z",
     "start_time": "2019-12-11T12:15:05.577331Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_sig_M (RDD):\n",
    "    maplist = []\n",
    "    for i in range(bands):\n",
    "        maplist.append( (RDD[0],(i+1,hash((RDD[1][2*i],RDD[1][2*i+1])))))\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:15:05.906525Z",
     "start_time": "2019-12-11T12:15:05.902540Z"
    }
   },
   "outputs": [],
   "source": [
    "Hash_buckets_M = Min_Hash_Data.flatMap(to_sig_M).map(lambda x: (x[1],[x[0]])) # (j,hash_buckets),[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pairs reduce by buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 用同一個band而且同一個bucket來reduce，將columns加入到同一個list裡面\n",
    "+ 去掉長度為`1`的list\n",
    "+ 將剩下的list用`itertools.combinations`來產生兩兩一對的`pairs tuple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:15:09.174250Z",
     "start_time": "2019-12-11T12:15:09.147323Z"
    }
   },
   "outputs": [],
   "source": [
    "same_bucket_list = Hash_buckets_M.reduceByKey(lambda x,y : x+y).values().filter(lambda x: len(x)>1)\n",
    "pairs = same_bucket_list.flatMap(lambda x: list(itertools.combinations(x,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:15:10.483662Z",
     "start_time": "2019-12-11T12:15:10.454738Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_count = pairs.map(lambda x: (x,1)).reduceByKey(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:15:35.063211Z",
     "start_time": "2019-12-11T12:15:23.883925Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((8, 34), 1),\n",
       " ((28, 30), 1),\n",
       " ((40, 14), 11),\n",
       " ((13, 49), 1),\n",
       " ((15, 39), 1),\n",
       " ((16, 50), 1),\n",
       " ((4, 49), 1),\n",
       " ((48, 49), 16),\n",
       " ((45, 26), 1),\n",
       " ((30, 35), 22),\n",
       " ((38, 23), 6),\n",
       " ((33, 50), 2),\n",
       " ((16, 17), 1),\n",
       " ((49, 47), 25),\n",
       " ((12, 20), 50),\n",
       " ((24, 40), 1),\n",
       " ((14, 30), 1),\n",
       " ((5, 43), 1),\n",
       " ((13, 47), 1),\n",
       " ((10, 18), 1),\n",
       " ((4, 47), 1),\n",
       " ((48, 47), 10),\n",
       " ((28, 35), 1),\n",
       " ((36, 39), 1),\n",
       " ((40, 47), 1)]"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_count.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting candidate pairs with min-hash values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 為了減少複雜度，所以先將原本的`hash values`篩選出`candidate pairs`內有的元素再處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:15:42.185448Z",
     "start_time": "2019-12-11T12:15:35.122050Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_1 = list(pairs_count.map(lambda x: x[0][0]).collect())\n",
    "pairs_2 = list(pairs_count.map(lambda x: x[0][1]).collect())\n",
    "pairs_12 = list(pairs_count.map(lambda x: x[0]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:15:42.238303Z",
     "start_time": "2019-12-11T12:15:42.234314Z"
    }
   },
   "outputs": [],
   "source": [
    "columns_1 = Min_Hash_Data.filter(lambda x : x[0] in pairs_1)\n",
    "columns_2 = Min_Hash_Data.filter(lambda x : x[0] in pairs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:15:42.313103Z",
     "start_time": "2019-12-11T12:15:42.298145Z"
    }
   },
   "outputs": [],
   "source": [
    "similar_pairs = columns_1.cartesian(columns_2).map(lambda x: ( (x[0][0],x[1][0]), (x[0][1],x[1][1]) )).filter(lambda x: x[0] in pairs_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:16:23.878879Z",
     "start_time": "2019-12-11T12:15:42.358981Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_pairs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Jaccard Simularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 計算相似度= 相同的min-hash value數量 / 全部的hash values數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:16:23.938718Z",
     "start_time": "2019-12-11T12:16:23.928746Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_similarity(RDD):\n",
    "    same_count = 0\n",
    "    for i in range(hash_functions):\n",
    "        if RDD[1][0][i]==RDD[1][1][i]:\n",
    "            same_count += 1\n",
    "    return (RDD[0],same_count/hash_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:16:23.981604Z",
     "start_time": "2019-12-11T12:16:23.976618Z"
    }
   },
   "outputs": [],
   "source": [
    "Similarity = similar_pairs.map(count_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 輸出前10高的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:18:29.360170Z",
     "start_time": "2019-12-11T12:16:24.040447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((12, 20), 1.0),\n",
       " ((49, 47), 0.73),\n",
       " ((30, 35), 0.65),\n",
       " ((48, 49), 0.55),\n",
       " ((38, 23), 0.46),\n",
       " ((40, 14), 0.41),\n",
       " ((48, 47), 0.4),\n",
       " ((16, 50), 0.09),\n",
       " ((33, 50), 0.09),\n",
       " ((45, 26), 0.08)]"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = Similarity.map(lambda x : (x[1],x[0])).sortByKey(False).map(lambda x : (x[1],x[0])).take(TopK)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T12:18:47.949006Z",
     "start_time": "2019-12-11T12:18:47.939003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\t20\t1.0\n",
      "\n",
      "49\t47\t0.73\n",
      "\n",
      "30\t35\t0.65\n",
      "\n",
      "48\t49\t0.55\n",
      "\n",
      "38\t23\t0.46\n",
      "\n",
      "40\t14\t0.41\n",
      "\n",
      "48\t47\t0.4\n",
      "\n",
      "16\t50\t0.09\n",
      "\n",
      "33\t50\t0.09\n",
      "\n",
      "45\t26\t0.08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('output.txt',\"w\")\n",
    "for item in ans:\n",
    "    line = str(item[0][0]) + \"\\t\" + str(item[0][1]) + \"\\t\" + str(item[1]) + \"\\n\"\n",
    "    print(line)\n",
    "    f.write(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 300.69234800000004,
   "position": {
    "height": "51.4743px",
    "left": "4.09486px",
    "right": "20px",
    "top": "472.98px",
    "width": "249.996px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
